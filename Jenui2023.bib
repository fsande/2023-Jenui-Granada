
BibTeX file
note            BibTeX-file
                Para ordenar ficheros bibtex con bibliografía, usar:
  
                bibtool -r /home/sande/bin/sande_bibtool.rsc -i entrada.bib -s -v -o salida.bib
  
                El fichero sande_bibtool.rsc está en /home/sande/bin/sande_bibtool.rsc
author          F. de Sande, fsande AT ull.es,
version         1.00,
date            23 April 2003,
filename        /home/sande/BIBLIOGRAFIA.bib,
address         Departamento de EIO y Computación
                Universidad de La Laguna
                38271 La Laguna, S/C de Tenerife,
                Canary Islands,
                Spain,
telephone       +34 922 318178,
FAX             +34 922 318170,
URL             http://webpages.ull.es/users/fsande,
keywords        bibliography, BibTeX, Parallel, OpenMP, MPI, Internet, Linux,,
Last update     2008/04/29 Ignorar comentarios, pretty printing
vim:ts=2:sw=2:expandtab:tw=80:fo=tcroq
vim:set fileencoding=utf8:set nu
====================================================================
@Article{Petit:Jutge:2018,
  author = {Jordi Petit and Josep Carmona and Jordi Cortadella and Jordi Duch and Omer Giménez and Anaga Mani and Jan Mas and Enric Rodriguez-Carbonell},
  title = {Jutge.org: Characteristics and Experiences},
  journal = {IEEE Transactions on Learning Technologies},
  volume = {11},
  number = {3},
  pages = {321--333},
  doi = {http://dx.doi.org/10.1109/TLT.2017.2723389},
  abstract = {The IEEE Transactions on Learning Technologies},
  url = {https://ieeexplore.ieee.org/document/7968379},
  year = {2018},
}

@Misc{URL::Jutge,
  key           = {Jutge},
  title         = {{Jutge.org Home Page}},
  note          = {\href{https://jutge.org/}
                  {\small{\texttt{https://jutge.org/}}}},
}

@Misc{URL::prob,
  key           = {Jutge},
  title         = {{Jutge.org Problems}},
  note          = {\href{https://jutge.org/problems/}
                  {\small{\texttt{https://jutge.org/problems/}}}},
}

@Misc{ULL:2022:GD,
  author        = {Universidad de La Laguna},
  institution   = {Universidad de La Laguna},
  title         = {Guía Docente de {I}nformática {B}ásica},
  year          = {2022},
  note          = {\href{https://www.ull.es/apps/guias/guias/view_guide/34182/}
                  {\small{\texttt{https://www.ull.es/apps/guias/guias/
                    \\view\_guide/34182}}}},
}

@Article{Zhang:2020:chatgpt,
  title={{ChatGPT}: A Task-Oriented Dialogue System Based on Pre-trained Language Model},
  author={Xingxing Zhang and Jinchao Zhang and Maxine Eskenazi},
  journal={arXiv preprint arXiv:2012.16641},
  year={2020}
}

@Article{Castelvecchi:2022:ACaA,
  author = {Davide Castelvecchi},
  title = {Are {ChatGPT} and {AlphaCode} going to replace programmers?},
  journal = {Nature (London)},
  issn = {0028-0836},
  year = {2022},
  month = {diciembre},
  doi = {10.1038/d41586-022-04383-z},
  language = {eng},
  address = {England},
}

@Article{Bao:2021:CAR,
  title={Conversational {AI}: A Review of the State of the Art},
  author={Siqi Bao and Jinchao Zhang and Xingxing Zhang and Maxine Eskenazi},
  journal={arXiv preprint arXiv:2104.07720},
  year={2021}
}

@misc{Friedman:2021:IGC,
  title={Introducing {GitHub Copilot}: your {AI} pair programmer},
  author={Nat Friedman},
  year={2021}
}

@Article{Chen:2021:ELL,
  title={Evaluating large language models trained on code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@Article{Li:2022:CCG,
  author = {Yujia Li and David Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and R{\'{e} }mi Leblond and Tom Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and Thomas Hubert and Peter Choy and Cyprien de Masson d'Autume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey Cherepanov and James Molloy and Daniel J. Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de Freitas and Koray Kavukcuoglu and Oriol Vinyals}, 
  title = {Competition-level code generation with {AlphaCode}},
  journal = {Science},
  publisher = {American Association for the Advancement of Science ({AAAS})},
  volume = {378},
  number = {6624},
  pages = {1092--1097},
  year = {2022},
  month = {diciembre},
  doi = {10.1126/science.abq1158},
  URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. 
              Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. 
              Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, 
              such as competitive programming problems. 
              Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations 
              on recent programming competitions on the Codeforces platform. 
              AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then 
              filtering and clustering those programs to a maximum of just 10 submissions. 
              This result marks the first time an artificial intelligence system has performed competitively in programming competitions. 
              Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions 
              to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. 
              Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model 
              that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous 
              participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. 
              It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one 
              responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.},
}

@Misc{Google::GSG,
  key           = {Google},
  author        = {Google}, 
  year          = {2023},
  title         = {{Google C++ Style Guide}},
  note          = {\href{https://google.github.io/styleguide/cppguide.html}
                  {\small{\texttt{https://google.github.io/styleguide/
                          \\cppguide.html}}}},
}

@Article{Perez:2022:FBM,
  title         = {``Funciona muy bien, pero no es magia'': así es {ChatGPT}, la nueva inteligencia artificial que supera límites},
  author        = {Jordi Pérez-Colomé},
  journal       = {El País},
  day           = {7},
  month         = {diciembre},
  year          = {2022},
  note          = {\href{https://elpais.com/tecnologia/2022-12-07/funciona-muy-bien-pero-no-es-magia-asi-es-chatgpt-la-nueva-inteligencia-artificial-que-supera-limites.html}
                  {\small{\texttt{https://elpais.com/}}}},
}

@Article{Cassidy:2023:AUR,
  title         = {Australian universities to return to 'pen and paper' exams after students caught using {AI} to write essays},
  author        = {Caitlin Cassidy},
  journal       = {The Guardian},
  day           = {10},
  month         = {enero},
  year          = {2023},
  note          = {\href{https://www.theguardian.com/australia-news/2023/jan/10/universities-to-return-to-pen-and-paper-exams-after-students-caught-using-ai-to-write-essays}
                  {\small{\texttt{https://www.theguardian.com/}}}},
}

@article{Dakhel:2022:GCA,
  title         = {{GitHub Copilot AI pair programmer: Asset or Liability?}},
  author        = {Arghavan Moradi-Dakhel and Vahid Majdinasab and Amin Nikanjam and Foutse Khomh and Michel C Desmarais and Zhen Ming and Jiang},
  year          = {2022},
  abstract = {Automatic program synthesis is a long-lasting dream in software engineering.
Recently, a promising Deep Learning (DL) based solution, called Copilot, has
been proposed by Open AI and Microsoft as an industrial product. Although some
studies evaluate the correctness of Copilot solutions and report its issues,
more empirical evaluations are necessary to understand how developers can
benefit from it effectively. In this paper, we study the capabilities of
Copilot in two different programming tasks: (1) generating (and reproducing)
correct and efficient solutions for fundamental algorithmic problems, and (2)
comparing Copilot's proposed solutions with those of human programmers on a set
of programming tasks. For the former, we assess the performance and
functionality of Copilot in solving selected fundamental problems in computer
science, like sorting and implementing basic data structures. In the latter, a
dataset of programming problems with human-provided solutions is used. The
results show that Copilot is capable of providing solutions for almost all
fundamental algorithmic problems, however, some solutions are buggy and
non-reproducible. Moreover, Copilot has some difficulties in combining multiple
methods to generate a solution. Comparing Copilot to humans, our results show
that the correct ratio of human solutions is greater than Copilot's correct
ratio, while the buggy solutions generated by Copilot require less effort to be
repaired. While Copilot shows limitations as an assistant for developers
especially in advanced programming tasks, as highlighted in this study and
previous ones, it can generate preliminary solutions for basic programming
tasks.},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
  language = {eng},
}

@article{Nguyen:2022:AnEE,
  title   = {{An Empirical Evaluation of GitHub Copilot's Code Suggestions}},
  author  = {Nhan Nguyen and Sarah Nadi},
  journal = {2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)},
  year    = {2022},
  pages   = {1-5}
}

@InProceedings{Kung:2022:PCU,
  author = {Tiffany H. Kung and Morgan Cheatham and ChatGPT and Arielle Medenilla and Czarina Sillos and Lorie De Leon and Camille Elepa{\~n}o and Maria Madriaga and Rimel Aggabao and Giezel Diaz-Candido and James Maningo and Victor Tseng},
  title = {{Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models}},
  year = {2022},
  doi = {10.1101/2022.12.19.22283643},
  publisher = {Cold Spring Harbor Laboratory Press},
  abstract = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study did not receive any external fundingAuthor DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesI confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesThe data analyzed in this study were obtained from USMLE sample questions sets which are publicly available. The question index, raw inputs, and raw AI outputs are available in the Online Data Supplement. Inquiries and requests for additional dataset items and adjudication results can be provided upon reasonable request by contacting Victor Tseng, MD (victor{at}ansiblehealth.com).ACIAccuracy-Concordance-Insight scoring systemDOIDensity of insightGPTGenerative pretrained transformerLLMLarge language modelMCSAMultiple choice single answerMC-JMultiple choice single answer with forced justificationMC-NJMultiple choice single answer without forced justificationNLPNatural language processingOEOpen-ended question formulationQn.mQuestion n, input run mUSMLEUnited States Medical Licensing Exam},
  URL = {https://www.medrxiv.org/content/early/2022/12/21/2022.12.19.22283643},
  eprint = {https://www.medrxiv.org/content/early/2022/12/21/2022.12.19.22283643.full.pdf},
  booktitle = {NPJ Digit. Med.},
  journal = {medRxiv}
}

@article{Floridi:2020:GPT-3,
  author = {Luciano Floridi and Massimo Chiriatti},
  title = {{GPT-3}: Its Nature, Scope, Limits, and Consequences},
  year = {2020},
  issue_date = {Dec 2020},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {30},
  number = {4},
  issn = {0924-6495},
  url = {https://doi.org/10.1007/s11023-020-09548-1},
  doi = {10.1007/s11023-020-09548-1},
  journal = {Minds Mach.},
  month = {diciembre},
  pages = {681–694},
  numpages = {14},
  keywords = {GPT-3, Automation, Turing Test, Irreversibility, Artificial Intelligence, Semantics},
  abstract = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, 
              questions that may enable one to identify the nature of the source of their answers. 
              We then introduce GPT-3, a third-generation, autoregressive language model that uses deep 
              learning to produce human-like texts, and use the previous distinction to analyse it. 
              We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), 
              and ethical questions and show that GPT-3 is not designed to pass any of them. 
              This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 
              as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. 
              We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.},
}

@article{Welsh:2023:TEoP,
  title = {The End of Programming},
  author = {Matt Welsh},
  journal = {Communications of the ACM},
  year = {2023},
  volume = {66},
  number = {1},
  pages = {34--35},
  issn = {0001-0782},
  abstract = {The end of classical computer science is coming, and most of us are dinosaurs waiting for the meteor to hit.},
  language = {eng},
}

%XXX 
% 
