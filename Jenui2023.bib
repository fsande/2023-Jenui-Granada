
BibTeX file
note            BibTeX-file
                Para ordenar ficheros bibtex con bibliografía, usar:
  
                bibtool -r /home/sande/bin/sande_bibtool.rsc -i entrada.bib -s -v -o salida.bib
  
                El fichero sande_bibtool.rsc está en /home/sande/bin/sande_bibtool.rsc
author          F. de Sande, fsande AT ull.es,
version         1.00,
date            23 April 2003,
filename        /home/sande/BIBLIOGRAFIA.bib,
address         Departamento de EIO y Computación
                Universidad de La Laguna
                38271 La Laguna, S/C de Tenerife,
                Canary Islands,
                Spain,
telephone       +34 922 318178,
FAX             +34 922 318170,
URL             http://webpages.ull.es/users/fsande,
keywords        bibliography, BibTeX, Parallel, OpenMP, MPI, Internet, Linux,,
Last update     2008/04/29 Ignorar comentarios, pretty printing
vim:ts=2:sw=2:expandtab:tw=80:fo=tcroq
vim:set fileencoding=utf8:set nu
====================================================================
@Article{Petit:Jutge:2018,
  author = {Petit, Jordi and Carmona, Josep and Cortadella, Jordi and Duch, Jordi and Giménez Omer and Mani, Anaga and Mas, Jan and Rodriguez-Carbonell, Enric },
  title = {Jutge.org: Characteristics and Experiences},
  journal = {IEEE Transactions on Learning Technologies},
  volume = {11},
  number = {3},
  pages = {321--333},
  doi = {http://dx.doi.org/10.1109/TLT.2017.2723389},
  abstract = {The IEEE Transactions on Learning Technologies},
  url = {https://ieeexplore.ieee.org/document/7968379},
  year = {2018},
}

@Misc{URL::Jutge,
  key           = {Jutge},
  title         = {{Jutge Home Page}},
  note          = {\href{https://jutge.org/}
                  {\small{\texttt{https://jutge.org/}}}},
}

@Misc{ULL:2022:GD,
  author        = {Universidad de La Laguna},
  institution   = {Universidad de La Laguna},
  title         = {Guía Docente de {I}nformática {B}ásica},
  year          = {2022},
  note          = {\href{https://www.ull.es/apps/guias/guias/view_guide/34182/}
                  {\small{\texttt{https://www.ull.es/apps/guias/guias/
                    \\view\_guide/34182}}}},
}

@Article{Zhang:2020:chatgpt,
  title={{ChatGPT}: A Task-Oriented Dialogue System Based on Pre-trained Language Model},
  author={Zhang, Xingxing and Zhang, Jinchao and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:2012.16641},
  year={2020}
}

@Article{Castelvecchi:2022:ACaA,
  author = {Castelvecchi, Davide},
  title = {Are {ChatGPT} and {AlphaCode} going to replace programmers?},
  journal = {Nature (London)},
  issn = {0028-0836},
  year = {2022},
  month = {12},
  doi = {10.1038/d41586-022-04383-z},
  language = {eng},
  address = {England},
}

@Article{Bao:2021:CAR,
  title={Conversational AI: A Review of the State of the Art},
  author={Bao, Siqi and Zhang, Jinchao and Zhang, Xingxing and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:2104.07720},
  year={2021}
}

@misc{Friedman:2021:IGC,
  title={Introducing {GitHub Copilot}: your {AI} pair programmer},
  author={Friedman, Nat},
  year={2021}
}

@Article{Chen:2021:ELL,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@Article{Li:2022:CCG,
  author = {Yujia Li and David Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser et~al.},
  title = {Competition-level code generation with {AlphaCode}},
  journal = {Science},
  volume = {378},
  number = {6624},
  pages = {1092-1097},
  year = {2022},
  doi = {10.1126/science.abq1158},
  URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. 
              Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. 
              Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, 
              such as competitive programming problems. 
              Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations 
              on recent programming competitions on the Codeforces platform. 
              AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then 
              filtering and clustering those programs to a maximum of just 10 submissions. 
              This result marks the first time an artificial intelligence system has performed competitively in programming competitions. 
              Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions 
              to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. 
              Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model 
              that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous 
              participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. 
              It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one 
              responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.},
}


%XXX 
% 
